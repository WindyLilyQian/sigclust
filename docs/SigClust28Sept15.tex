%Packages
\documentclass{article}
\usepackage[backend=bibtex]{biblatex}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage[all]{xy}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{tikz}

%Formatting

\hyphenation{mar-gin-al-ia}
%

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}


%Title
\title{A walk through the SigClust algorithm}
\date{}
\author{}


%MAIN DOCUMENT
\begin{document}

\maketitle

%

In a 2008 paper\footnote{Yufeng Liu, David Neil Hayes, Andrew Nobel and J. S. Marron, \emph{Statistical Significance of Clustering for High-Dimension, Low-Sample Size Data} Journal of the American Statistical Association, Vol. 103, No. 483 (Sep., 2008), pp. 1281-1293\\ See also http://arxiv.org/abs/1305.5879v2}
Liu et al. present the clustering algorithm SigClust. The algorithm takes as a framework the assumption that clusters are by definition, generated by single, multivariate Gaussian distributions. Thus the algorithm considers the null hypothesis that the data in question (viewed as the rows of $X\in \mathbb{R}^{m \times n}$) come from a single, multivariate Gaussian.  A translation and rotation invariant test static ($CI_{2}(\cdot)$, the $k$-means cluster index for $k=2$) is used, and thus we can assume without loss that the distribution of the null hypothesis is distributed as $D_{0} \sim N(0, \Sigma_{0})$ where $\Sigma_{0}$ is a diagonal matrix. This matrix is estimated from data, taking into account some symmetric (fixed $\sigma^{2}$), multivariate Gaussian background noise. Given $D_0$, one can estimate a distribution on the test statistic $CI_{k}(Z)$ on datasets $Z$ generated by $D_0$. From here it is straightforward to compute the p-value of the $CI_{k}$ of the clustered \emph{original} dataset with respect to this CDF. When the p-value is below some threshold $\alpha$, the null hypothesis is rejected. The data may then be clustered as desired, in particular one may use the two clusters given when computing $CL_{2}(X)$.\\

\indent Here we will present a short tour of SigClust. Since Liu et al. break the algorithm down into seven steps, we will devote a small section to each step (after making some initial definitions). \\

\begin{definition}[Preliminary]
Throughout let $X\in \mathbb{R}^{m\times n}$ be a data matrix for a dataset with $m$ samples and $n$ features.
So for each $1 \leq i \leq m$, the $i$th \emph{row}
$$X^{i} := X(i,\cdot)$$
gives all the feature values for the $i$th \emph{sample}, and the for each $1\leq i \leq n$ the $i$th \emph{column}
$$X_{i} := X(\cdot, i)$$
 gives a vector of data for the $i$th \emph{feature}.
Also, let 
$$rows(X):= \{X^{i}: 1\leq i \leq m \}$$
refer to the set of all $m$ of the $1\times n$ row vectors of $X$ and let 
$$cols(X):=\{X_{i}: 1\leq i \leq n\}$$ denote the $n$, $m\times 1$ column vectors of $X$.
\end{definition}

\begin{definition}[The $k$-means Cluster Index ($CI_{k}$)]
\label{CI}
Let $k, m, n\in \mathbb{Z}^{+}$ (positive integers).

Given $X \in \mathbb{R}^{m \times n}$, the $k$-means algorithm delivers a partition $\mathscr{Q}$ of $rows(X)$ with $\vert \mathscr{Q} \vert = k$ and determines a corresponding partition
$$\mathscr{P}_{k}(X) := \langle C_1, C_2, \ldots, C_k \rangle$$ 
of the \emph{index set} $I = \{1, 2, \ldots, m\}$.

 Given the latter partition, we define the \emph{$k$-means cluster index} for $X$ as
$$CI_{k}(X) = \frac{\sum_{j = 1}^{k} \sum_{i \in C_{j}} \Vert X^{i} - c_{j}\Vert^{2}}{\sum_{i = 1}^{m} \Vert X^{i} - \bar{X}\Vert^{2}}$$
where
$\Vert \cdot \Vert$ here is just the Euclidean norm for one dimensional (row or column) vectors,
$$\bar{X}:= mean(rows(X)),$$
 and for each $1\leq i \leq k$
$$c_i := mean(X(C_{i}, \cdot))$$
is the mean of all rows of $X$ with row index in $C_i$.

\end{definition}

\indent Lin et. al. note that ``The smaller the CI, the larger the proportion of the overall variance that is explained by clustering.''  \\
\indent Also, it is easy to see that this function is invariant under translation and isometric transformation of the rows of the input data matrix.

\begin{lemma}  Let $1_{m}$ be the $m \times 1$ column vector of ones.\\
For any data matrix $X\in \mathbb{R}^{m\times n}$, any row vector $\mu \in \mathbb{R}^{n}$, and any isometry $M\in \mathscr{O}(n)$ we have
$$CI_{k}(XM + 1_{m}\mu) = CI_{k}(X)$$
\end{lemma}
\begin{proof}(Partial)
This should be geometrically clear, but we give a partial proof of invariance under isometric transformation and leave invariance under translation to the reader.  Let $M\in \mathscr{O}_{n}$, and let 
$$Y = XM = \left[ \begin{array}{c} X^{1} \\ X^{2} \\ \vdots \\ X^{m} \end{array} \right] \left[ \begin{array}{cccc} M_{1} & M_{2} & \cdots & M_{n} \end{array} \right]$$
so that $Y(i, j) = X^{i}M_{j}$ for all $1\leq i \leq m$ and $1\leq j \leq n$.
First notice that the when we apply $k$-means to $X$ and to $Y$ we have
$$\mathscr{P}_{k}(X) = \langle C_1, C_2, \ldots, C_k \rangle= \mathscr{P}_{k}(Y)$$
(from Definition \ref{CI}). 
Now let $1\leq i \leq m$ and consider the $k$th component $(Y^{i} - \bar{Y})_{k}$ of the difference $(Y^{i} - \bar{Y})$. We see that
$$(Y^{i} - \bar{Y})_{k} = X^{i}M_{k} - \frac{1}{m}\sum_{r=1}^{m} Y(r, k) = X^{i}M_{k} - \frac{1}{m}\sum_{r=1}^{m} X^{r}M_{k} $$
$$= (X^{i} - \frac{1}{m}\sum_{r=1}^{m} X^{r})M_{k} = (X^{i} - \bar{X})M_{k}.$$
Thus
$$Y^{i} - \bar{Y} =  \left[ \begin{array}{cccc} (X^{i}- \bar{X}) M_{1} & (X^{i}- \bar{X}) M_{2} & \cdots & (X^{i}- \bar{X}) M_{n} \end{array} \right] = (X^{i}- \bar{X})M,$$
and so the deviation is just 
$$\Vert Y^{i} - \bar{Y} \Vert = \Vert (X^{i} - \bar{X}) M \Vert =  \Vert ((X^{i} - \bar{X}) M)^{T} \Vert  = \vert M^{T} (X^{i} - \bar{X})^T \Vert = \Vert (X^{i} - \bar{X})^T \Vert  = \Vert X^{i} - \bar{X}\Vert $$
where the second to last equality follows since $M$ (and hence $M^{T}$) is an isometry.  This shows that the denominators of $CI_{k}(X)$ and $CI_{k}(Y)$ from Definition \ref{CI} are the same; the cases for each term in the numerators are proved similarly.
\end{proof}








We will also need

\begin{definition}[Median absolute deviation from the median (MAD)]
Let $Y$ be a univariate (scalar-valued) data set.  Then we define
$$MAD(Y) := median(\vert Y - median(Y)\vert).$$
\end{definition}


\section{The SigClust Algorithm}
Let $m, n \in \mathbb{Z}^{+}$ and $X\in \mathbb{R}^{m \times n}$.

%Step 1
\subsection{Step 1:  Compute $CI_{2}(X)$}
See above definition, and use $k=2$.  This value will not be needed until the final step $7$, so it can be computed at any point. 

%Step 2
\subsection{Step 2: Compute background noise level $\sigma_{N}^2$}  In order to apply some dimensionality reduction later, we assume some level $\epsilon \sim N(0, \sigma^{2}_{N} I_{n})$ background noise on the data. We estimate the standard deviation $\sigma_{N}$ by

$$\sigma_{N} = \frac{MAD(X_{F})}{\Phi^{-1}(\frac{3}{4})}=\frac{MAD(X_{F})}{.6745} = (1.48257969) MAD(X_{F})$$

where $X_{F}$ is a flattened (univariate) representation of the data $X$ and $\Phi^{-1}$ is the inverse CDF of the standard normal $N(0, 1)$.


%Step 3
\subsection{Step 3: PCA }
We compute the eigenvalues (with multiplicities) for the covariance matrix for the mean-shifted data.  Let's break this into steps.
\begin{enumerate}

\item
Let $\mu \in \mathbb{R}^{n}$ be the column vector $\mu = \langle \mu_{1}, \ldots, \mu_{n}\rangle$ where for $1\leq i \leq n$, $\mu_i$ is the mean of the $i$th feature, that is, the mean of column $X_i = X(\cdot, i)$.  
Also let $1_{n} = \langle1, \dots, 1 \rangle$ be the column vector of all 1s. The mean centered data is given by
$$Y = X - 1_{n} \mu^{T}.$$

\item Compute the covariance matrix for the data matrix $Y$ by 
$$\Sigma = \left(\frac{1}{n-1}\right)Y^{T}Y.$$  Using $n-1$ instead of $n$ here is Bessel's correction.  More on this in the future.

\item Compute the eigenvalues of $\Sigma$.  Strictly speaking we only need the eigenvalues and not the eigenvectors for the rest of the algorithm.  But note that $\Sigma$ is a real symmetric matrix and thus has is diagonalizable by a orthogonal matrix $M$ (a rigid rotation about the origin).  In particular, $\Sigma$ has an eigendecomposition
$$\Sigma = MDM^{T}$$ where 
$$D = diag(\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n}),$$
$$M = [v_1 v_2 \ldots v_n],$$
the $\lambda_{i}$ are the eigenvalues of $\Sigma$ in non-increasing order, and the $v_i$ are corresponding eigenvectors of length $\Vert v_{i} \Vert_{2} = 1$ which are pairwise orthogonal ($v_i \cdot v_j = 0$ for all $i\neq j$).\\
\indent Note that we are only going to use eigenvalues greater than $\sigma^2_{N}$ from step 2, so using a method (such as Raleigh quotients) to compute eigenvalues from largest to smallest could save computation.

\item Upon finding the necessary eigenvalues $\lambda_{i}$, for each $1\leq i \leq n$, set $$\hat{\lambda}_{i} = max(\lambda_{i}, \sigma_{N}^{2}).$$

\item Define $$\Sigma_{0} = diag(\hat{\lambda}_{1}\ldots, \hat{\lambda}_{n})$$

\item We now have a mean-centered, axis-aligned multivariate Gaussian distribution $$N(0, \Sigma_{0})$$ from which to simulate data. We will refer to this distribution as the \emph{null distribution}.

\end{enumerate}
We now begin the "Monte Carlo" part of this algorithm.

%Step 4 SImulation
\subsection{Step 4: Simulation}
 Generate a data set $Z\in \mathbb{R}^{m \times n}$ from the null distribution.  If this is the $t$th iteration of the Simulation step we can call this data set $Z_t$.
 
 %Step 5  Clustering
\subsection{Step 5: Clustering}  Compute $z = CL_{2}(Z)$ where $Z$ is from the previous step.  If this is the $t$th iteration of the Clustering step we can call this value $z_t = CI_{2}(Z_t)$.

 %Step 6
\subsection{Step 6: Iterate, then compute distribution on $CL_{2}$}  
Repeat the previous two steps $N_{L}$ times for some large $N_{L}$.  Let $S:=\{z_{t}\}_{1\leq t\leq N_{L}}.$\\
%Step 7
\subsection{Step 7:  Compute $p$-value of the test statistic}
\indent We can now estimate the CDF of $CL_{2}(Z)$ for $Z\in \mathbb{R}^{m \times n}$ with rows generated by $D_{0} \sim N(0, \Sigma_0)$.  Specifically, for any $r\in \mathbb{R}$ we estimate that
$$P_{Z\sim D_{0}}(CL_{2}(Z) \leq r) = \frac{1}{N_{L}} \lvert \{ t: 1 \leq t \leq N_{L}, z_t \leq r \} \vert.$$
In particular, letting $r=r_X: = CL_{2}(X)$ from step 1, we can compute the $p$-value $$P_{Z\sim D_{0}}(CL_{2}(Z) \leq r_{X}).$$
If this $p$-value is smaller than some threshold $\alpha$, we reject the null hypothesis and split  $rows(X)$ into two clusters according to the clusters discovered in step 1.   We may then repeat the SigClust algorithm on either or both of the two clusters.

















\end{document}